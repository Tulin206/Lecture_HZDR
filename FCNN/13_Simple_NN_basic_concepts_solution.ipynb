{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/Tulin206/Lecture_HZDR/blob/main/FCNN/13_Simple_NN_basic_concepts_solution.ipynb)"
   ],
   "id": "be8b77ce6cd1cca5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  ðŸ§  A Neural Network for the Iris Dataset (Solution)\n",
    "#\n",
    "# This notebook builds a neural network to classify the famous **Iris dataset** using only **NumPy**.\n",
    "#\n",
    "# **Key concepts:**\n",
    "# - One-Hot Encoding\n",
    "# - Softmax Activation\n",
    "# - Cross-Entropy Loss\n",
    "# - Data Scaling\n"
   ],
   "id": "243a9025b3257d83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n"
   ],
   "id": "12a4015f57e4f274"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ],
   "id": "8e3eedc8393e6ae7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1: Load and Preprocess Data\n",
   "id": "cc8163f0269d580c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load Data\n",
    "iris = load_iris()\n",
    "X_raw = iris.data      # (150 samples, 4 features)\n",
    "y_raw = iris.target    # (150 samples,) containing 0, 1, 2\n"
   ],
   "id": "ded37c713f09f078"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define Helper Functions\n",
    "\n",
    "def to_one_hot(y, num_classes):\n",
    "    \"\"\"Converts (150,) array of ints to (150, 3) one-hot matrix\"\"\"\n",
    "    one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    for i, label in enumerate(y):\n",
    "        one_hot[i, label] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def min_max_scale(X):\n",
    "    \"\"\"Scales features to be between 0 and 1\"\"\"\n",
    "    min_val = X.min(axis=0)\n",
    "    max_val = X.max(axis=0)\n",
    "    return (X - min_val) / (max_val - min_val)\n"
   ],
   "id": "c3d8f6bdd25cf3df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocess Data\n",
    "X = min_max_scale(X_raw)         # Scale features to [0, 1]\n",
    "\n",
    "# One-hot encode the outputs\n",
    "num_classes = 3\n",
    "y = to_one_hot(y_raw, num_classes)\n"
   ],
   "id": "f2e49adee80cc404"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Shuffle the data (important because Iris is sorted by class!)\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "print(\"First 5 X samples (scaled):\\n\", X[:5])\n",
    "print(\"\\nFirst 5 y samples (one-hot):\\n\", y[:5])\n"
   ],
   "id": "6cab3da5d10b8198"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2: Advanced Activation & Loss\n",
   "id": "44c5a230ad303d30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    # Subtract max for numerical stability (prevents blowing up exp)\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n"
   ],
   "id": "91b7287a89a958b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: Network Architecture\n",
   "id": "b170ebde2eeaed7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_neurons = 4\n",
    "hidden_neurons = 6\n",
    "output_neurons = 3\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 5000\n",
    "\n",
    "# Initialize weights (standard normal distribution usually works better here)\n",
    "# We multiply by 0.1 to keep initial weights small\n",
    "\n",
    "weights_hidden = np.random.randn(input_neurons, hidden_neurons) * 0.1\n",
    "bias_hidden = np.zeros((1, hidden_neurons))\n",
    "\n",
    "weights_output = np.random.randn(hidden_neurons, output_neurons) * 0.1\n",
    "bias_output = np.zeros((1, output_neurons))\n"
   ],
   "id": "818952c2fc3162ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 4: The Training Loop\n",
   "id": "a37af3b684c82db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    # 1. FORWARD PASS\n",
    "    hidden_layer_input = np.dot(X, weights_hidden) + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_output) + bias_output\n",
    "    # USE SOFTMAX NOW\n",
    "    predicted_output = softmax(output_layer_input)\n",
    "\n",
    "    # 2. LOSS (Cross-Entropy for monitoring)\n",
    "    # Small epsilon to prevent log(0) errors\n",
    "    epsilon = 1e-15\n",
    "    clipped_preds = np.clip(predicted_output, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(np.sum(y * np.log(clipped_preds), axis=1))\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # 3. BACKWARD PASS\n",
    "    # Gradient of Cross-Entropy + Softmax is just (Pred - Actual)\n",
    "    d_predicted_output = (predicted_output - y) / X.shape[0] # Normalize by batch size\n",
    "\n",
    "    # Backprop to hidden layer\n",
    "    error_hidden = d_predicted_output.dot(weights_output.T)\n",
    "    d_hidden_layer = error_hidden * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # 4. UPDATE WEIGHTS\n",
    "    weights_output -= hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    bias_output -= np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "    weights_hidden -= X.T.dot(d_hidden_layer) * learning_rate\n",
    "    bias_hidden -= np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(f\"Epoch {i} Loss: {loss:.4f}\")\n"
   ],
   "id": "f4af48b73a824d71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 5: Testing Accuracy\n",
   "id": "f798bae20d879692"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Final forward pass to get predictions\n",
    "hidden_out = sigmoid(np.dot(X, weights_hidden) + bias_hidden)\n",
    "final_preds_prob = softmax(np.dot(hidden_out, weights_output) + bias_output)\n",
    "\n",
    "# Convert probabilities to class labels (0, 1, or 2)\n",
    "predicted_classes = np.argmax(final_preds_prob, axis=1)\n",
    "true_classes = np.argmax(y, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "print(f\"\\nFinal Training Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Show a few examples\n",
    "print(\"\\nSample Predictions vs True Labels:\")\n",
    "print(f\"Predicted: {predicted_classes[:10]}\")\n",
    "print(f\"True:      {true_classes[:10]}\")\n"
   ],
   "id": "ac704a5c06496bbc"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
